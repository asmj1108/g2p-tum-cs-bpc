#!/bin/bash
# Sweeps over data and hyperparameters.

set -euo pipefail

# Script usage
usage() {
	cat <<EOF
Usage: $0 [OPTIONS] [MODE]

OPTIONS:
    --help, -h      Show this help message

MODE:
    tune            Run hyperparameter tuning on 'ady' language
    (default)       Run training on all languages with best hyperparameters

Examples:
    $0 tune         # Run hyperparameter tuning
    $0              # Run training on all languages
EOF
}

# Defaults.
readonly SEED=1917
readonly CRITERION=label_smoothed_cross_entropy
readonly LABEL_SMOOTHING=.1
readonly OPTIMIZER=adam
readonly LR=1e-3
readonly LR_SCHEDULER=inverse_sqrt
readonly WARMUP_INIT_LR=1e-7
readonly WARMUP_UPDATES=1000
readonly CLIP_NORM=1.
readonly MAX_UPDATE=5000
readonly SAVE_INTERVAL=5
readonly ENCODER_LAYERS=4
readonly ENCODER_ATTENTION_HEADS=4
readonly DECODER_LAYERS=4
readonly DECODER_ATTENTION_HEADS=4
readonly ACTIVATION_FN=relu

# Hyperparameters to be tuned.
readonly BATCHES=(256 512 1024)
readonly DROPOUTS=(.1 .2 .3)

# Prediction options.
readonly BEAM=5

LANGS=('ara' 'arm_e' 'asm' 'bel' 'ben' 'bul' 'bur' 'ceb' 'dut' 'eng_us' 'fas' 'fre' 'geo' 'ger' 'gle'
	'gre' 'hbs_latn' 'hin' 'hun' 'ice' 'ind' 'ita' 'jpn_hira' 'khm' 'kor' 'lav' 'lit' 'lwl' 'mkd' 'mlt_latn' 'nno' 'per'
	'pus' 'rum' 'rus' 'shn' 'slv' 'spa' 'swe' 'tgl' 'tha' 'ukr' 'urd' 'vie_hanoi' 'wel' 'wel_sw')

MODE="default"

# Parse command line arguments
parse_args() {
	while [[ $# -gt 0 ]]; do
		case $1 in
		--help | -h)
			usage
			exit 0
			;;
		tune)
			MODE="tune"
			shift
			;;
		*)
			echo "Error: Unknown argument $1" >&2
			usage >&2
			exit 1
			;;
		esac
	done
}

train() {
	local -r CP="$1"
	shift
	fairseq-train \
		"data-bin/${LANGUAGE}" \
		--save-dir="${CP}" \
		--source-lang="${LANGUAGE}.graphemes" \
		--target-lang="${LANGUAGE}.phonemes" \
		--disable-validation \
		--seed="${SEED}" \
		--arch=transformer \
		--attention-dropout="${DROPOUT}" \
		--activation-dropout="${DROPOUT}" \
		--activation-fn="${ACTIVATION_FN}" \
		--encoder-embed-dim="${EED}" \
		--encoder-ffn-embed-dim="${EHS}" \
		--encoder-layers="${ENCODER_LAYERS}" \
		--encoder-attention-heads="${ENCODER_ATTENTION_HEADS}" \
		--encoder-normalize-before \
		--decoder-embed-dim="${DED}" \
		--decoder-ffn-embed-dim="${DHS}" \
		--decoder-layers="${DECODER_LAYERS}" \
		--decoder-attention-heads="${DECODER_ATTENTION_HEADS}" \
		--decoder-normalize-before \
		--share-decoder-input-output-embed \
		--criterion="${CRITERION}" \
		--label-smoothing="${LABEL_SMOOTHING}" \
		--optimizer="${OPTIMIZER}" \
		--lr="${LR}" \
		--lr-scheduler="${LR_SCHEDULER}" \
		--warmup-init-lr="${WARMUP_INIT_LR}" \
		--warmup-updates="${WARMUP_UPDATES}" \
		--clip-norm="${CLIP_NORM}" \
		--batch-size="${BATCH}" \
		--max-update="${MAX_UPDATE}" \
		--save-interval="${SAVE_INTERVAL}" \
		"$@" # In case we need more configuration control.
}

evaluate() {
	local -r CP="$1"
	shift
	local -r MODE="$1"
	shift
	# Fairseq insists on calling the dev-set "valid"; hack around this.
	local -r FAIRSEQ_MODE="${MODE/dev/valid}"
	shopt -s nullglob
	for CHECKPOINT in "${CP}"/checkpoint[1-9]*.pt; do
		RES="${CHECKPOINT/.pt/-${MODE}.res}"
		# Don't overwrite an existing prediction file.
		if [[ -f "${RES}" ]]; then
			continue
		fi
		echo "Evaluating into ${RES}"
		OUT="${CP}/${MODE}.out"
		TSV="${CP}/${MODE}.tsv"
		# Makes raw predictions.
		fairseq-generate \
			"data-bin/${LANGUAGE}" \
			--source-lang="${LANGUAGE}.graphemes" \
			--target-lang="${LANGUAGE}.phonemes" \
			--path="${CHECKPOINT}" \
			--seed="${SEED}" \
			--gen-subset="${FAIRSEQ_MODE}" \
			--beam="${BEAM}" \
			--no-progress-bar \
			>"${OUT}"
		# Extracts the predictions into a TSV file.
		paste \
			<(grep '^T-' "${OUT}" | cut -f2) \
			<(grep '^H-' "${OUT}" | cut -f3) \
			>"${TSV}"
		# Applies the evaluation script to the TSV file.
		python evaluation/evaluate.py "${TSV}" >"${RES}" 2>/dev/null
		# Cleans up intermediate files.
		rm -f "${OUT}" "${TSV}"
	done
	shopt -u nullglob
}

train_evaluate() {
	local -r CP="$1"
	shift
	train "${CP}"
	evaluate "${CP}" dev
	evaluate "${CP}" test
	./inspect "${CP}" clean
}

# These set the encoder and/or decoder size.

small_encoder() {
	export EED=128
	export EHS=512
}

large_encoder() {
	export EED=256
	export EHS=1024
}

small_decoder() {
	export DED=128
	export DHS=512
}

large_decoder() {
	export DED=256
	export DHS=1024
}

run_hyperparameter_tuning() {
	# Choose a language for hyperparameter tuning process
	local -r LANGUAGE="ady"
	for BATCH in "${BATCHES[@]}"; do
		for DROPOUT in "${DROPOUTS[@]}"; do
			small_encoder
			small_decoder
			train_evaluate "checkpoints/${LANGUAGE}-${BATCH}-${DROPOUT}-s-s"
			small_encoder
			large_decoder
			train_evaluate "checkpoints/${LANGUAGE}-${BATCH}-${DROPOUT}-s-l"
			large_encoder
			small_decoder
			train_evaluate "checkpoints/${LANGUAGE}-${BATCH}-${DROPOUT}-l-s"
			large_encoder
			large_decoder
			train_evaluate "checkpoints/${LANGUAGE}-${BATCH}-${DROPOUT}-l-l"
		done
	done
}

run_training() {
	local -r BATCH=512
	local -r DROPOUT=.3
	for LANGUAGE in "${LANGS[@]}"; do
		large_encoder
		small_decoder
		train_evaluate "checkpoints/${LANGUAGE}-${BATCH}-${DROPOUT}-l-s"
	done
}

parse_args "$@"
if [ "${MODE}" == "tune" ]; then
	run_hyperparameter_tuning
else
	run_training
fi
